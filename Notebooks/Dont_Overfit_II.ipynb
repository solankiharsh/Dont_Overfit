{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, time, sys, h5py\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from keras.layers import Input, Dense , Dropout , TimeDistributed , LSTM , GRU, concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD , Adadelta, RMSprop, Adam, Adamax\n",
    "from keras.models import  load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import  to_categorical \n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize problem parameters\n",
    "class Args:\n",
    "    \"\"\" Class containing all model arguments \"\"\"\n",
    "    def __init__( self ):\n",
    "        self.project    = 'MLchallenge_DontOverfit'\n",
    "        self.dataPath   = '/home/harsh/Downloads/DontOverfit/'       .format(self.project)\n",
    "        self.modelsPath = '/home/harsh/Downloads/DontOverfit/Models/' .format(self.project)\n",
    "        self.resultsPath= '/home/harsh/Downloads/DontOverfit/Results/'.format(self.project)\n",
    "        self.CV_folds   = 40  # split the Training data in stratified folds, to train different versions of models \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                f0           f1           f2           f3           f4  \\\n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000   \n",
      "mean      0.000566     0.000697     0.000468     0.001733     0.000708   \n",
      "std       0.019962     0.024577     0.016497     0.031072     0.024959   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.704060     0.866833     0.581853     0.709016     0.880315   \n",
      "\n",
      "                f5           f6           f7           f8           f9  ...  \\\n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000  ...   \n",
      "mean      0.000717     0.000585     0.000357     0.007151     0.000693  ...   \n",
      "std       0.025296     0.020650     0.012606     0.050962     0.024434  ...   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "max       0.892193     0.728320     0.444614     0.526275     0.861779  ...   \n",
      "\n",
      "             f1247        f1248        f1249        f1250        f1251  \\\n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000   \n",
      "mean      0.003622     0.000915     0.000867     0.000433     0.000528   \n",
      "std       0.038963     0.023519     0.021617     0.015265     0.018627   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.544915     0.712600     0.554228     0.538398     0.656990   \n",
      "\n",
      "             f1252        f1253        f1254        f1255        label  \n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000  \n",
      "mean      0.000434     0.000721     0.000477     0.000496     5.167203  \n",
      "std       0.015313     0.025446     0.016820     0.017501     3.662910  \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "50%       0.000000     0.000000     0.000000     0.000000     6.000000  \n",
      "75%       0.000000     0.000000     0.000000     0.000000     9.000000  \n",
      "max       0.540103     0.897483     0.593238     0.617260     9.000000  \n",
      "\n",
      "[8 rows x 1257 columns]\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "train = pd.read_csv( args.dataPath + 'TTT_train.csv' )\n",
    "test = pd.read_csv( args.dataPath + 'TTT_test_features.csv', index_col = 'ID')\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_iforest.py:252: FutureWarning: 'behaviour' is deprecated in 0.22 and will be removed in 0.24. You should not pass or set this parameter.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove outlier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "isf = IsolationForest(contamination='auto', behaviour='new', n_jobs=-1)\n",
    "isf.fit(train.drop('label', axis=1), train['label'])\n",
    "y_train_outlier = isf.predict(train.drop('label', axis=1))\n",
    "train = train[np.where(y_train_outlier == 1, True, False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train.drop('label', axis=1), train[\"label\"].values, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: lr  0.7941176470588235\n",
      "Accuracy score: clf  0.8850267379679144\n",
      "Accuracy score: mnb  0.7165775401069518\n",
      "Accuracy score: gnb  0.5374331550802139\n",
      "Accuracy score: knn  0.6390374331550802\n",
      "Accuracy score: rf  0.7887700534759359\n",
      "Accuracy score: svc  0.7299465240641712\n"
     ]
    }
   ],
   "source": [
    "#Training the model and Testing Accuracy on Validation data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters = {'penalty':['l2'], 'C': np.arange(0.05, 1.05, 0.05)}\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1, multi_class='auto', solver='lbfgs', class_weight='balanced', max_iter=10000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "clf = GridSearchCV(lr, parameters, cv=9)\n",
    "clf.fit(train.drop('label', axis=1), train['label'])\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1, n_neighbors=9)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "svc = SVC(gamma='scale', decision_function_shape='ovo')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_val_lr = lr.predict(X_val)\n",
    "print('Accuracy score: lr ', accuracy_score(y_val, y_val_lr))\n",
    "y_val_clf = clf.predict(X_val)\n",
    "print('Accuracy score: clf ', accuracy_score(y_val, y_val_clf))\n",
    "y_val_mnb = mnb.predict(X_val)\n",
    "print('Accuracy score: mnb ', accuracy_score(y_val, y_val_mnb))\n",
    "y_val_gnb = gnb.predict(X_val)\n",
    "print('Accuracy score: gnb ', accuracy_score(y_val, y_val_gnb))\n",
    "y_val_knn = knn.predict(X_val)\n",
    "print('Accuracy score: knn ', accuracy_score(y_val, y_val_knn))\n",
    "y_val_rf = rf.predict(X_val)\n",
    "print('Accuracy score: rf ', accuracy_score(y_val, y_val_rf))\n",
    "y_val_svc = svc.predict(X_val)\n",
    "print('Accuracy score: svc ', accuracy_score(y_val, y_val_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = train.drop('label', axis=1).values\n",
    "y_train_all = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Downloading mlxtend-0.17.3-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.2 in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.18.5)\n",
      "Requirement already satisfied: setuptools in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (49.6.0.post20200814)\n",
      "Requirement already satisfied: joblib>=0.13.2 in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (0.23.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (3.3.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /home/harsh/anaconda3/lib/python3.8/site-packages (from mlxtend) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/harsh/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.20.3->mlxtend) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/harsh/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.2->mlxtend) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/harsh/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.2->mlxtend) (2020.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/harsh/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/harsh/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/harsh/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (7.2.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /home/harsh/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/harsh/anaconda3/lib/python3.8/site-packages (from matplotlib>=3.0.0->mlxtend) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/harsh/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->mlxtend) (1.15.0)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install mlxtend  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: logisticregression (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: sgdclassifier (3/3)\n",
      "Accuracy score: sc  1.0\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, RidgeClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb = xgb.XGBClassifier(verbosity=1,\n",
    "                        n_jobs=-1,\n",
    "                        objective='multi:softprob', \n",
    "                        n_estimators=500,\n",
    "                        max_depth=3)\n",
    "\n",
    "params = {'meta-logisticregression__C': [0.001, 0.01, 0.1, 1, 10.0, 100]}\n",
    "\n",
    "sc = StackingClassifier(\n",
    "    classifiers=[\n",
    "        LogisticRegression(penalty='l2', n_jobs=-1, multi_class='auto', solver='lbfgs', max_iter=10000),\n",
    "        RandomForestClassifier(n_estimators=500, n_jobs=-1),\n",
    "        SGDClassifier(loss='log', max_iter=1000, tol=1e-3)\n",
    "    ],\n",
    "    verbose=1,\n",
    "    use_probas=True,\n",
    "    meta_classifier=LogisticRegression(penalty='l2', n_jobs=-1, multi_class='auto', solver='lbfgs', max_iter=10000)\n",
    ")\n",
    "\n",
    "sc.fit(X_train_all, y_train_all)\n",
    "\n",
    "y_val_sc = sc.predict(X_val)\n",
    "print('Accuracy score: sc ', accuracy_score(y_val, y_val_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.get_params().keys()\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slight param tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ngrid = GridSearchCV(estimator=sc, \\n                    param_grid=params, \\n                    cv=4)\\ngrid.fit(X_train_all, y_train_all)\\n\\ny_val_grid = grid.predict(X_val)\\nprint('Accuracy score: grid ', accuracy_score(y_val, y_val_grid))\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "grid = GridSearchCV(estimator=sc, \n",
    "                    param_grid=params, \n",
    "                    cv=4)\n",
    "grid.fit(X_train_all, y_train_all)\n",
    "\n",
    "y_val_grid = grid.predict(X_val)\n",
    "print('Accuracy score: grid ', accuracy_score(y_val, y_val_grid))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: logisticregression (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: sgdclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: logisticregression (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: sgdclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: logisticregression (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: sgdclassifier (3/3)\n",
      "Fitting 3 classifiers...\n",
      "Fitting classifier1: logisticregression (1/3)\n",
      "Fitting classifier2: randomforestclassifier (2/3)\n",
      "Fitting classifier3: sgdclassifier (3/3)\n",
      "Accuracy: 0.78 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "score = sklearn.model_selection.cross_val_score(sc, train.drop('label', axis=1).values, train['label'].values, cv=4, scoring='accuracy')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = sc.predict(test)\n",
    "result = test.reset_index()[['ID']].copy()\n",
    "result['label'] = y_test_pred\n",
    "\n",
    "result.to_csv(path_or_buf= args.dataPath + 'mle_tiny_submission.csv' , encoding='utf-8', index=False, header=['ID', 'label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
