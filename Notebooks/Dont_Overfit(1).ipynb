{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoid overfitting with a tiny sliver for training data\n",
    "\n",
    "Inspired by the Kaggle Don’t Overfit Challenge: Tiny Training Trial. The challenge; build the best performing model you can with a <5% training vs >95% test split with TF-IDF encodings on an Amazon multi-classification problem. With so many data hungry algorithms out there that take days or more to compute, we thought it’d be refreshing to go the other way and experiment with what can be done with extremely small and noisy datasets! Iterate and experiment with training times on the order of seconds. Our split is:\n",
    "\n",
    "Train: 1244 points\n",
    "\n",
    "Approach overview\n",
    "\n",
    "•Build Ensemble that includes multiple model categories: Logistic Regression, Random Forests, XGBoost, Adaboost, and Neural Networks.\n",
    "\n",
    "•Split the training dataset into K stratified folds. For each fold and model category, train a separate model using Grid Search.\n",
    "\n",
    "•Combine all models into ensemble using Averaging.\n",
    "\n",
    "I experimented with:\n",
    "\n",
    "    1.Which model categories to include in the ensemble\n",
    "    2.How many stratified folds to use: 1, 5, 10, 20, 40\n",
    "    3.How to build the ensemble: Averaging vs. Max voting\n",
    "    4.Oversampling techniques such as SMOTE and ADASYN: including models trained with SMOTE data in the ensemble worked for the Public leaderboad, but not for Private\n",
    "    5.Feature standardization: did not seem to improve anything.\n",
    "\n",
    "Lessons Learned\n",
    "\n",
    "    Ensembling is the way to go, of course.\n",
    "    Increasing the number of stratified folds improved performance.\n",
    "    Improvements in training data accuracy (on validation set) did not necessarrily translate to better accuracies in the Public dataset. A prime example for this was the LR method that did not perform as well in the training validation accuracy compared to other methods such as NN. However, LR was an integral part of the overall Ensemble; whenever we removed it, the Public dataset accuracy ended up much worse.\n",
    "    Ensembling using Averaging always worked better than Max voting.\n",
    "    We kind of `overfitted' to the Public Leaderboard, i.e., our best performing model in Public was not the best in Private.\n",
    "    Adding models trained with oversampled data, using either SMOTE or ADASYN, decreased accuracy in Private dataset.\n",
    "    Gini impurity appeared to work better than Entropy for tree-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, time, sys, h5py\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from keras.layers import Input, Dense , Dropout , TimeDistributed , LSTM , GRU, concatenate, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD , Adadelta, RMSprop, Adam, Adamax\n",
    "from keras.models import  load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import  to_categorical \n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize problem parameters\n",
    "class Args:\n",
    "    \"\"\" Class containing all model arguments \"\"\"\n",
    "    def __init__( self ):\n",
    "        self.project    = 'MLchallenge_DontOverfit'\n",
    "        self.dataPath   = '/home/harsh/Downloads/DontOverfit/'       .format(self.project)\n",
    "        self.modelsPath = '/home/harsh/Downloads/DontOverfit/Models/' .format(self.project)\n",
    "        self.resultsPath= '/home/harsh/Downloads/DontOverfit/Results/'.format(self.project)\n",
    "        self.CV_folds   = 40  # split the Training data in stratified folds, to train different versions of models \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                f0           f1           f2           f3           f4  \\\n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000   \n",
      "mean      0.000566     0.000697     0.000468     0.001733     0.000708   \n",
      "std       0.019962     0.024577     0.016497     0.031072     0.024959   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.704060     0.866833     0.581853     0.709016     0.880315   \n",
      "\n",
      "                f5           f6           f7           f8           f9  ...  \\\n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000  ...   \n",
      "mean      0.000717     0.000585     0.000357     0.007151     0.000693  ...   \n",
      "std       0.025296     0.020650     0.012606     0.050962     0.024434  ...   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
      "max       0.892193     0.728320     0.444614     0.526275     0.861779  ...   \n",
      "\n",
      "             f1247        f1248        f1249        f1250        f1251  \\\n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000   \n",
      "mean      0.003622     0.000915     0.000867     0.000433     0.000528   \n",
      "std       0.038963     0.023519     0.021617     0.015265     0.018627   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       0.544915     0.712600     0.554228     0.538398     0.656990   \n",
      "\n",
      "             f1252        f1253        f1254        f1255        label  \n",
      "count  1244.000000  1244.000000  1244.000000  1244.000000  1244.000000  \n",
      "mean      0.000434     0.000721     0.000477     0.000496     5.167203  \n",
      "std       0.015313     0.025446     0.016820     0.017501     3.662910  \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "50%       0.000000     0.000000     0.000000     0.000000     6.000000  \n",
      "75%       0.000000     0.000000     0.000000     0.000000     9.000000  \n",
      "max       0.540103     0.897483     0.593238     0.617260     9.000000  \n",
      "\n",
      "[8 rows x 1257 columns]\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "train = pd.read_csv( args.dataPath + 'TTT_train.csv' )\n",
    "test = pd.read_csv( args.dataPath + 'TTT_test_features.csv', index_col = 'ID')\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 36 members, which is less than n_splits=40.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    }
   ],
   "source": [
    "X = train.loc[:, train.columns != 'label']\n",
    "y = train['label']\n",
    "y_cat = to_categorical(y)\n",
    "# Generate a set of stratified folds of Training to train different versions of each model.\n",
    "folds = list(StratifiedKFold(n_splits=args.CV_folds, shuffle=True, random_state=1).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# function to fit a model on every fold, and store trained model\n",
    "def fitValidateSave( model, modelType ):\n",
    "    #\n",
    "    accuracies = []\n",
    "    for foldIndex, fold in enumerate(folds):\n",
    "        X_fold      = np.take( X, fold[0], axis=0)\n",
    "        y_fold      = np.take( y, fold[0], axis=0)\n",
    "        #\n",
    "        #oversampler = RandomOverSampler(random_state=77)\n",
    "        #X_fold, y_fold = oversampler.fit_sample(X_fold, y_fold)\n",
    "        #\n",
    "        X_fold_test = np.take( X, fold[1], axis=0)\n",
    "        y_fold_test = np.take( y, fold[1], axis=0)\n",
    "        #\n",
    "        model.fit(X_fold, y_fold)\n",
    "        #\n",
    "        accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
    "        print( '{}: {}'.format(foldIndex, accuracies[-1]) )\n",
    "        #print(model.best_params_)\n",
    "        #\n",
    "        pickle.dump( model, open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldIndex ) , 'wb'))\n",
    "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
    "    return model\n",
    "##################################################\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Compute accuracies across folds using an already trained model.\n",
    "def validateAcrossFolds( modelType ):\n",
    "    #\n",
    "    accuracies = []\n",
    "    for foldInd, fold in enumerate(folds):\n",
    "        X_fold_test = np.take( X, fold[1], axis=0)\n",
    "        y_fold_test = np.take( y, fold[1], axis=0)\n",
    "        #\n",
    "        if 'NN' in modelType:\n",
    "            y_fold_test = to_categorical(y_fold_test)\n",
    "            model = load_model( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ) )\n",
    "            accuracies.append( model.evaluate(X_fold_test, y_fold_test, batch_size=512, verbose=0 )[1] )\n",
    "        else:\n",
    "            model = pickle.load(open( '{}/{}_fold{}.h5'.format( args.modelsPath, modelType, foldInd ), 'rb'))\n",
    "            accuracies.append( model.score(X_fold_test, y_fold_test) )\n",
    "        print( '{}: {}'.format(foldInd, accuracies[-1]) )\n",
    "        #\n",
    "    print( 'Average accuracy for {} is:  {}'.format( modelType, np.mean(accuracies)) )  \n",
    "    return model\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.75\n",
      "1: 0.78125\n",
      "2: 0.8125\n",
      "3: 0.8125\n",
      "4: 0.8709677419354839\n",
      "5: 0.7096774193548387\n",
      "6: 0.8709677419354839\n",
      "7: 0.7741935483870968\n",
      "8: 0.9032258064516129\n",
      "9: 0.7741935483870968\n",
      "10: 0.8709677419354839\n",
      "11: 0.7741935483870968\n",
      "12: 0.7741935483870968\n",
      "13: 0.9354838709677419\n",
      "14: 0.7419354838709677\n",
      "15: 0.7419354838709677\n",
      "16: 0.7741935483870968\n",
      "17: 0.8387096774193549\n",
      "18: 0.6774193548387096\n",
      "19: 0.7419354838709677\n",
      "20: 0.9032258064516129\n",
      "21: 0.8064516129032258\n",
      "22: 0.6774193548387096\n",
      "23: 0.7419354838709677\n",
      "24: 0.8064516129032258\n",
      "25: 0.7741935483870968\n",
      "26: 0.8709677419354839\n",
      "27: 0.7419354838709677\n",
      "28: 0.7419354838709677\n",
      "29: 0.8387096774193549\n",
      "30: 0.8709677419354839\n",
      "31: 0.7096774193548387\n",
      "32: 0.8709677419354839\n",
      "33: 0.8709677419354839\n",
      "34: 0.7096774193548387\n",
      "35: 0.8064516129032258\n",
      "36: 0.8387096774193549\n",
      "37: 0.7419354838709677\n",
      "38: 0.7419354838709677\n",
      "39: 0.7419354838709677\n",
      "Average accuracy for LR is:  0.793422379032258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\":[\"l2\"],\n",
    "    \"C\": [ 3., 4., 5.],\n",
    "    \"fit_intercept\": [True],\n",
    "    \"class_weight\":['balanced'],\n",
    "    \"solver\":[ 'lbfgs' ],\n",
    "    \"multi_class\": [\"multinomial\"],\n",
    "    \"random_state\":[77]\n",
    "    }\n",
    "LR = GridSearchCV(LogisticRegression(), \n",
    "                  parameters, \n",
    "                  cv=4, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "LR = fitValidateSave( LR, 'LR' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.8125\n",
      "1: 0.78125\n",
      "2: 0.84375\n",
      "3: 0.84375\n",
      "4: 0.9032258064516129\n",
      "5: 0.7419354838709677\n",
      "6: 0.8387096774193549\n",
      "7: 0.7419354838709677\n",
      "8: 0.9354838709677419\n",
      "9: 0.7096774193548387\n",
      "10: 0.8387096774193549\n",
      "11: 0.7741935483870968\n",
      "12: 0.8709677419354839\n",
      "13: 0.9354838709677419\n",
      "14: 0.7741935483870968\n",
      "15: 0.8387096774193549\n",
      "16: 0.7741935483870968\n",
      "17: 0.8387096774193549\n",
      "18: 0.7419354838709677\n",
      "19: 0.7419354838709677\n",
      "20: 0.8709677419354839\n",
      "21: 0.7741935483870968\n",
      "22: 0.7419354838709677\n",
      "23: 0.7096774193548387\n",
      "24: 0.7096774193548387\n",
      "25: 0.7096774193548387\n",
      "26: 0.8709677419354839\n",
      "27: 0.8387096774193549\n",
      "28: 0.8064516129032258\n",
      "29: 0.8709677419354839\n",
      "30: 0.8387096774193549\n",
      "31: 0.7096774193548387\n",
      "32: 0.8064516129032258\n",
      "33: 0.8064516129032258\n",
      "34: 0.7741935483870968\n",
      "35: 0.7419354838709677\n",
      "36: 0.9032258064516129\n",
      "37: 0.7741935483870968\n",
      "38: 0.6774193548387096\n",
      "39: 0.8387096774193549\n",
      "Average accuracy for RF_gini is:  0.8013860887096774\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"criterion\":[\"gini\"],\n",
    "    \"max_depth\":[ 15, 30  ],\n",
    "    \"min_samples_split\": [ 5 ],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"max_features\":[None ],\n",
    "    \"random_state\": [77],\n",
    "    \"n_estimators\":[ 200 ]\n",
    "    }\n",
    "RF_gini = GridSearchCV(RandomForestClassifier(), \n",
    "                  parameters, \n",
    "                  cv=4, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "RF_gini = fitValidateSave( RF_gini, 'RF_gini' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.8125\n",
      "1: 0.8125\n",
      "2: 0.8125\n",
      "3: 0.875\n",
      "4: 0.8064516129032258\n",
      "5: 0.7741935483870968\n",
      "6: 0.8709677419354839\n",
      "7: 0.7741935483870968\n",
      "8: 0.967741935483871\n",
      "9: 0.7741935483870968\n",
      "10: 0.8064516129032258\n",
      "11: 0.8387096774193549\n",
      "12: 0.8064516129032258\n",
      "13: 0.9032258064516129\n",
      "14: 0.8064516129032258\n",
      "15: 0.9032258064516129\n",
      "16: 0.8387096774193549\n",
      "17: 0.9032258064516129\n",
      "18: 0.7419354838709677\n",
      "19: 0.7419354838709677\n",
      "20: 0.9032258064516129\n",
      "21: 0.8064516129032258\n",
      "22: 0.7419354838709677\n",
      "23: 0.7419354838709677\n",
      "24: 0.8387096774193549\n",
      "25: 0.7741935483870968\n",
      "26: 0.8387096774193549\n",
      "27: 0.8387096774193549\n",
      "28: 0.8387096774193549\n",
      "29: 0.7741935483870968\n",
      "30: 0.8064516129032258\n",
      "31: 0.7096774193548387\n",
      "32: 0.7741935483870968\n",
      "33: 0.9354838709677419\n",
      "34: 0.8387096774193549\n",
      "35: 0.7419354838709677\n",
      "36: 0.8709677419354839\n",
      "37: 0.7741935483870968\n",
      "38: 0.7096774193548387\n",
      "39: 0.7419354838709677\n",
      "Average accuracy for AB_gini is:  0.8142641129032258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "AB_gini = AdaBoostClassifier( base_estimator = DecisionTreeClassifier( \n",
    "                             criterion         = 'gini', \n",
    "                             splitter          = 'random',\n",
    "                             max_depth         = 30, \n",
    "                             min_samples_split = 5, \n",
    "                             min_samples_leaf  = 1,\n",
    "                             max_features      = None,\n",
    "                             random_state      = 77 \n",
    "                            ),\n",
    "                            learning_rate= 1,\n",
    "                            n_estimators = 200\n",
    "                         )\n",
    "AB_gini = fitValidateSave( AB_gini, 'AB_gini' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:16:07] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "0: 0.78125\n",
      "[18:16:49] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "1: 0.75\n",
      "[18:17:33] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "2: 0.78125\n",
      "[18:18:15] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "3: 0.875\n",
      "[18:18:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "4: 0.8709677419354839\n",
      "[18:19:33] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "5: 0.7096774193548387\n",
      "[18:20:15] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "6: 0.8709677419354839\n",
      "[18:21:02] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "7: 0.7419354838709677\n",
      "[18:21:48] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "8: 0.9354838709677419\n",
      "[18:22:35] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "9: 0.7741935483870968\n",
      "[18:23:24] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "10: 0.9032258064516129\n",
      "[18:24:10] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "11: 0.7741935483870968\n",
      "[18:24:56] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "12: 0.9032258064516129\n",
      "[18:25:43] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "13: 0.967741935483871\n",
      "[18:26:30] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "14: 0.7419354838709677\n",
      "[18:27:17] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "15: 0.7419354838709677\n",
      "[18:28:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "16: 0.7419354838709677\n",
      "[18:28:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "17: 0.8387096774193549\n",
      "[18:29:40] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "18: 0.7419354838709677\n",
      "[18:30:28] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "19: 0.7741935483870968\n",
      "[18:31:16] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "20: 0.8387096774193549\n",
      "[18:32:05] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "21: 0.8064516129032258\n",
      "[18:32:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "22: 0.6774193548387096\n",
      "[18:33:45] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "23: 0.6451612903225806\n",
      "[18:34:35] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "24: 0.7419354838709677\n",
      "[18:35:25] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "25: 0.7419354838709677\n",
      "[18:36:15] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "26: 0.8064516129032258\n",
      "[18:37:07] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "27: 0.8064516129032258\n",
      "[18:37:58] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "28: 0.7741935483870968\n",
      "[18:38:49] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "29: 0.9354838709677419\n",
      "[18:39:38] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "30: 0.7741935483870968\n",
      "[18:40:30] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "31: 0.7096774193548387\n",
      "[18:41:22] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "32: 0.7096774193548387\n",
      "[18:42:15] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "33: 0.8709677419354839\n",
      "[18:43:07] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "34: 0.7741935483870968\n",
      "[18:44:02] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "35: 0.8064516129032258\n",
      "[18:44:56] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "36: 0.9032258064516129\n",
      "[18:45:50] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "37: 0.7741935483870968\n",
      "[18:46:44] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "38: 0.7096774193548387\n",
      "[18:47:39] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "39: 0.8709677419354839\n",
      "Average accuracy for XGB is:  0.797429435483871\n"
     ]
    }
   ],
   "source": [
    "XGB = XGBClassifier(  max_depth=6,  \n",
    "                      learning_rate=0.1, \n",
    "                      n_estimators=100, \n",
    "                      verbosity=1, \n",
    "                      objective='multi:softmax', \n",
    "                      num_class=y_cat.shape[-1],\n",
    "                      booster='gbtree', \n",
    "                      n_jobs=4, \n",
    "                      gamma=0, \n",
    "                      min_child_weight=1,\n",
    "                      max_delta_step=0, \n",
    "                      subsample=.7, \n",
    "                      colsample_bytree=.6, \n",
    "                      colsample_bylevel=.6, \n",
    "                      colsample_bynode=.6, \n",
    "                      reg_alpha=.0, \n",
    "                      reg_lambda=.0, \n",
    "                      scale_pos_weight=1, \n",
    "                      base_score=0.1, \n",
    "                      random_state=77 \n",
    "                      )\n",
    "XGB = fitValidateSave( XGB, 'XGB' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
